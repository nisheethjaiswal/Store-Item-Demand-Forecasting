{"cells":[{"metadata":{"_uuid":"6e2756c941afa8493f292ce8565a0eb18ab26ef7"},"cell_type":"markdown","source":"## Introduction\n\nARIMA is one of the most classic time series forecasting models. During the modeling process, we mainly want to find 3 parameters. Auto-regression(AR) term, namly the lags of previous value; Integral(I) term for non-stationary differencing and Moving Average(MA) for error term.  \n\nI'm a newbie in this field. Found many online tutorials used grid search technique(auto.arima in R). Meanwhile I also found many hypothesis test to validate the time series, i.e. see if it's stationary, looking at ACF and PACF to suggest a AR term etc... \n\nFacebook has a package called prophet, which is quite complex and consider many things automaticlly. But out of curiosity, I want to understand what's the reasoning behind the model. ARIMA is definitely a good starting point.\n\n### My goal for this notebook:\n1. Understand ARIMA, SARIMA, ARIMAX  \n2. Walkthrough the necessary tests that ARIMA needs to statisfy\n3. Find a set of reasonable parameters base on a statistic tests and visualizations   \n\n### Notebook Outline:\n* ARIMA introduction  \n* Decompose the ts\n* Stationarize the data  \n* Interpret ACF and PACF  \n* Determine p, d, q\n* Adding seasonality: S-ARMIA\n* Adding holiday factors to be SARIMA-X\n\n\n#### A few things on my TODO list:\n- mulitple seasonality\n- outlier detection"},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true,"_uuid":"1a0c06bd41ef79a5c8acdd54686993b2031e279b"},"cell_type":"code","source":"import warnings \nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\nimport statsmodels.api as sm\n\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43e701579b56f07b937e6261d7e90054f0647bf3"},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output(['ls', '../input']).decode('utf-8'))","execution_count":null,"outputs":[]},{"metadata":{"slideshow":{"slide_type":"slide"},"trusted":true,"_uuid":"07ac6c57769c08e59e849446ee1beaba37d66b36"},"cell_type":"code","source":"train = pd.read_csv('../input/demand-forecasting-kernels-only/train.csv')\ntrain['date'] = pd.to_datetime(train['date'], format=\"%Y-%m-%d\")\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80bd1451cc2c5bff776aa942e8a6dc4015f135c0"},"cell_type":"markdown","source":"# Forecast modeling - ARIMA\n\nwe want to start with some basic/classic model like armia. \nHere is a list of online tutorials that helps me get started:  \nhttp://www.statsmodels.org/dev/examples/notebooks/generated/tsa_arma_0.html  \nhttp://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/  \nhttp://barnesanalytics.com/basics-of-arima-models-with-statsmodels-in-python\n\nARIMA model includes the AR term, the I term, and the MA term. Let’s actually start with the I term, as it is the easiest to explain. The I term is a full difference. That is today’s value minus yesterday’s value. That’s it.  \nThe way that I like to think of the AR term is that it is a partial difference. The coefficient on the AR term will tell you the percent of a difference you need to take.   \n**MA**  \nA moving average term in a time series model is a past error (multiplied by a coefficient).The label \"moving average\" is is somewhat misleading because the weights 1,−θ1,−θ2,…,−θq, which multiply the a's, need not total unity nor need that be positive.  \nXt=εt+θ1εt−1+⋯+θqεt−q  as akin to a weighted moving average of the ε terms,\n"},{"metadata":{"trusted":true,"_uuid":"87df2b860733255fbc8e960f7f2ae3133f9b32a0"},"cell_type":"code","source":"# per 1 store, 1 item\ntrain_df = train[train['store']==1]\ntrain_df = train_df[train['item']==1]\n# train_df = train_df.set_index('date')\ntrain_df['year'] = train['date'].dt.year\ntrain_df['month'] = train['date'].dt.month\ntrain_df['day'] = train['date'].dt.dayofyear\ntrain_df['weekday'] = train['date'].dt.weekday\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2137a5e7470eb62663b74844b4d3bce41659aca"},"cell_type":"markdown","source":"## Decompose the time series\nTo start with, we want to decompose the data to seperate the seasonality, trend and residual. Since we have 5 years of sales data. We would expect there's a yearly or weekly pattern. Let's use a function in statsmodels to help us find it. "},{"metadata":{"trusted":true,"_uuid":"bd54caa7b6b5eb48553a329086bd0cc2129a5a84"},"cell_type":"code","source":"sns.lineplot(x=\"date\", y=\"sales\",legend = 'full' , data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9ab27bcbb096bd40e70fd9cc8c52ad939ad2ea3"},"cell_type":"code","source":"sns.lineplot(x=\"date\", y=\"sales\",legend = 'full' , data=train_df[:28])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b4ebbecaecac4ebf910c7643217a5d1ba9af56a"},"cell_type":"code","source":"sns.boxplot(x=\"weekday\", y=\"sales\", data=train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"472ade391a94feaa299d83f1e4c9a5b6156c7641"},"cell_type":"markdown","source":"Monday=0, Sunday=6.  \nHere we can find the weekends(5,6) has a larger sales, weekdays(0-4) are smaller. There's a few outliers on Monday, Wed."},{"metadata":{"trusted":true,"_uuid":"cd30773611a255d59e9a79be4588bae7a71e03fa"},"cell_type":"code","source":"train_df = train_df.set_index('date')\ntrain_df['sales'] = train_df['sales'].astype(float)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abde3372bd8924e9730495472ea150b2b6f2a1f6"},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(train_df['sales'], model='additive', freq=365)\n\nfig = plt.figure()  \nfig = result.plot()  \nfig.set_size_inches(15, 12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37fb860cb3aba17c0dae8c0266bdabc4a8bfb834"},"cell_type":"markdown","source":"Playing with a few frequency, the yearly pattern is very obvious. and also we can see a upwards trend. Which means this data is not stationary."},{"metadata":{"_uuid":"dcbfe22a324c83710e741f949ed47e2049a7e33b"},"cell_type":"markdown","source":"###  Stationarize the data:\n\nWhat does it mean for data to be stationary?\n\nThe mean of the series should not be a function of time. The red graph below is not stationary because the mean increases over time. \n![alt text](https://imgur.com/LjtBXwf.png)\n\nThe variance of the series should not be a function of time. This property is known as homoscedasticity. Notice in the red graph the varying spread of data over time. \n![alt text](https://imgur.com/v2Uye7X.png)\n\nFinally, the covariance of the i th term and the (i + m) th term should not be a function of time. In the following graph, you will notice the spread becomes closer as the time increases. Hence, the covariance is not constant with time for the ‘red series’. \n![Imgur](https://i.imgur.com/6HVlvg2.png)  \nWhy is this important? When running a linear regression the assumption is that all of the observations are all independent of each other. In a time series, however, we know that observations are time dependent. It turns out that a lot of nice results that hold for independent random variables (law of large numbers and central limit theorem to name a couple) hold for stationary random variables. So by making the data stationary, we can actually apply regression techniques to this time dependent variable.\n\nThere are two ways you can check the stationarity of a time series. The first is by looking at the data. By visualizing the data it should be easy to identify a changing mean or variation in the data. For a more accurate assessment there is the Dickey-Fuller test. I won’t go into the specifics of this test, but if the ‘Test Statistic’ is greater than the ‘Critical Value’ than the time series is stationary. Below is code that will help you visualize the time series and test for stationarity.\n\n"},{"metadata":{"trusted":true,"_uuid":"e9cbc13adbe2538ca0379751eb32c15bafeed898"},"cell_type":"code","source":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries, window = 12, cutoff = 0.01):\n\n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window).mean()\n    rolstd = timeseries.rolling(window).std()\n\n    #Plot rolling statistics:\n    fig = plt.figure(figsize=(12, 8))\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show()\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    pvalue = dftest[1]\n    if pvalue < cutoff:\n        print('p-value = %.4f. The series is likely stationary.' % pvalue)\n    else:\n        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n    \n    print(dfoutput)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6bb44be338102b24fb62ba8a92d69dd399e2492f"},"cell_type":"code","source":"test_stationarity(train_df['sales'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21a35cea79bd763a3399115e8672500f85e44953"},"cell_type":"markdown","source":"the smaller p-value, the more likely it's stationary. Here our p-value is 0.036. It's actually not bad, if we use a 5% Critical Value(CV), this series would be considered stationary. But as we just visually found an upward trend, we want to be more strict, we use 1% CV.  \nTo get a stationary data, there's many techiniques. We can use log, differencing etc... "},{"metadata":{"trusted":true,"_uuid":"ae1e5b9ccdf31a47b48e0d20bd25f2ceaa370725"},"cell_type":"code","source":"first_diff = train_df.sales - train_df.sales.shift(1)\nfirst_diff = first_diff.dropna(inplace = False)\ntest_stationarity(first_diff, window = 12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ef9dc951591af3b49fbcb361f117c5d528f0978"},"cell_type":"markdown","source":"After differencing, the p-value is extremely small. Thus this series is very likely to be stationary.  \n\n\n## ACF and PACF\n\nThe partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\n\n### Autoregression Intuition\nConsider a time series that was generated by an autoregression (AR) process with a lag of k.\n\nWe know that the ACF describes the autocorrelation between an observation and another observation at a prior time step that includes direct and indirect dependence information.\n\nThis means we would expect the ACF for the AR(k) time series to be strong to a lag of k and the inertia of that relationship would carry on to subsequent lag values, trailing off at some point as the effect was weakened.\n\nWe know that the PACF only describes the direct relationship between an observation and its lag. This would suggest that there would be no correlation for lag values beyond k.\n\nThis is exactly the expectation of the ACF and PACF plots for an AR(k) process.\n\n### Moving Average Intuition\nConsider a time series that was generated by a moving average (MA) process with a lag of k.\n\nRemember that the moving average process is an autoregression model of the time series of residual errors from prior predictions. Another way to think about the moving average model is that it corrects future forecasts based on errors made on recent forecasts.\n\nWe would expect the ACF for the MA(k) process to show a strong correlation with recent values up to the lag of k, then a sharp decline to low or no correlation. By definition, this is how the process was generated.\n\nFor the PACF, we would expect the plot to show a strong relationship to the lag and a trailing off of correlation from the lag onwards.\n\nAgain, this is exactly the expectation of the ACF and PACF plots for an MA(k) process.\n### Summary\nFrom the autocorrelation plot we can tell whether or not we need to add MA terms. From the partial autocorrelation plot we know we need to add AR terms.\n\n### References:  \nhttps://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/\n"},{"metadata":{"trusted":true,"_uuid":"3002d08b73a8aa34e458a5ed43ffe17d7256b64e"},"cell_type":"code","source":"import statsmodels.api as sm\n\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(train_df.sales, lags=40, ax=ax1) # \nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(train_df.sales, lags=40, ax=ax2)# , lags=40","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07d1c0a66c6bc9b3ccec6e7ed1c8c6690d4a9b97"},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(first_diff, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(first_diff, lags=40, ax=ax2)\n\n# Here we can see the acf and pacf both has a recurring pattern every 7 periods. Indicating a weekly pattern exists. \n# Any time you see a regular pattern like that in one of these plots, you should suspect that there is some sort of \n# significant seasonal thing going on. Then we should start to consider SARIMA to take seasonality into accuont","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"471105ce7f8551619eed9e8b0f36cdb4be0621f7"},"cell_type":"markdown","source":"Because the autocorrelation of the differenced series is negative at lag 7, 14, 21 etc.. (every week), I should an SMA term to the model.   \n## Build the model\n\n### How to determin p, d, q\n\nIt's easy to determin I. In our case, we see the first order differencing make the ts stationary. **I = 1**.\n\nAR model might be investigated first with lag length selected from the PACF or via empirical investigation. In our case, it's clearly that within 6 lags the AR is significant. Which means, we can use ** AR = 6** \n\nTo avoid the potential for incorrectly specifying the MA order (in the case where the MA is first tried then the MA order is being set to 0),  it may often make sense to extend the lag observed from the last significant term in the PACF.\n\nWhat is interesting is that when the AR model is appropriately specified, the the residuals from this model can be used to directly observe the uncorrelated error. This residual can be used to further investigate alternative MA and ARMA model specifications directly by regression.\n\nAssuming an AR(s) model were computed, then I would suggest that the next step in identification is to estimate an MA model with s-1 lags in the uncorrelated errors derived from the regression. The parsimonious MA specification might be considered and this might be compared with a more parsimonious AR specification. Then ARMA models might also be analysed.\n### Reference:\nhttps://www.researchgate.net/post/How_does_one_determine_the_values_for_ARp_and_MAq   \nhttps://stats.stackexchange.com/questions/281666/how-does-acf-pacf-identify-the-order-of-ma-and-ar-terms/281726#281726  \nhttps://stats.stackexchange.com/questions/134487/analyse-acf-and-pacf-plots?rq=1  "},{"metadata":{"trusted":true,"_uuid":"c0250aff147382899a50d89e2fd106bd636d8400"},"cell_type":"code","source":"arima_mod6 = sm.tsa.ARIMA(train_df.sales, (6,1,0)).fit(disp=False)\nprint(arima_mod6.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0409af01a11bb57a3fcef69a9887eec3abc2b80"},"cell_type":"markdown","source":"### Analyze the result\nTo see how our first model perform, we can plot the residual distribution. See if it's normal dist. And the ACF and PACF. For a good model, we want to see the residual is normal distribution. And ACF, PACF has not significant terms."},{"metadata":{"trusted":true,"_uuid":"7abf3684fb85164182834b6bd47bfbdec84b4b77"},"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import normaltest\n\nresid = arima_mod6.resid\nprint(normaltest(resid))\n# returns a 2-tuple of the chi-squared statistic, and the associated p-value. the p-value is very small, meaning\n# the residual is not a normal distribution\n\nfig = plt.figure(figsize=(12,8))\nax0 = fig.add_subplot(111)\n\nsns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(resid)\n\n#Now plot the distribution using \nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('Residual distribution')\n\n\n# ACF and PACF\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(arima_mod6.resid, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(arima_mod6.resid, lags=40, ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c749321852ee7fe54cf304eadb542da27ef05e8d"},"cell_type":"markdown","source":"Although the graph looks very like a normal distribution. But it failed the test. Also we see a recurring correlation exists in both ACF and PACF. So we need to deal with seasonality.\n\n### Consider seasonality affect by SARIMA\n\n\nhttps://www.statsmodels.org/dev/examples/notebooks/generated/statespace_sarimax_stata.html  \nhttps://barnesanalytics.com/sarima-models-using-statsmodels-in-python"},{"metadata":{"trusted":true,"_uuid":"10cd57caa5ade104670bc8fdac2066d18ba80c7d"},"cell_type":"code","source":"sarima_mod6 = sm.tsa.statespace.SARIMAX(train_df.sales, trend='n', order=(6,1,0)).fit()\nprint(sarima_mod6.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a059e1cee7d9c33e52abbe3cddd5e9d44c6d26eb"},"cell_type":"code","source":"resid = sarima_mod6.resid\nprint(normaltest(resid))\n\nfig = plt.figure(figsize=(12,8))\nax0 = fig.add_subplot(111)\n\nsns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(resid)\n\n#Now plot the distribution using \nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('Residual distribution')\n\n\n# ACF and PACF\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(arima_mod6.resid, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(arima_mod6.resid, lags=40, ax=ax2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21e83dc5ec8e4fd7b5f8614f99569d1c6e6e7e6b"},"cell_type":"markdown","source":"## Make prediction and evaluation\n\nTake the last 30 days in training set as validation data"},{"metadata":{"trusted":true,"_uuid":"31f10644a11609f44765eaa544716cbc5bd2718f"},"cell_type":"code","source":"start_index = 1730\nend_index = 1826\ntrain_df['forecast'] = sarima_mod6.predict(start = start_index, end= end_index, dynamic= True)  \ntrain_df[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2fc22d1f88d4b92ca087713eff7046dac271cf8"},"cell_type":"code","source":"def smape_kun(y_true, y_pred):\n    mape = np.mean(abs((y_true-y_pred)/y_true))*100\n    smape = np.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true))).fillna(0))\n    print('MAPE: %.2f %% \\nSMAPE: %.2f'% (mape,smape), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6be12c96e2ca369b3c861be7975b85bbb96e3c2a"},"cell_type":"code","source":"smape_kun(train_df[1730:1825]['sales'],train_df[1730:1825]['forecast'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"9e58f6b7ccf68df74c70c17bf4e0bf02daa9200a"},"cell_type":"markdown","source":"## SARIMAX: adding external variables "},{"metadata":{"trusted":true,"_uuid":"b1f589762e030e20fef29dca3cf821acb197c3f3"},"cell_type":"code","source":"# per 1 store, 1 item\nstoreid = 1\nitemid = 1\ntrain_df = train[train['store']==storeid]\ntrain_df = train_df[train_df['item']==itemid]\n\n# train_df = train_df.set_index('date')\ntrain_df['year'] = train_df['date'].dt.year - 2012\ntrain_df['month'] = train_df['date'].dt.month\ntrain_df['day'] = train_df['date'].dt.dayofyear\ntrain_df['weekday'] = train_df['date'].dt.weekday\n\nstart_index = 1730\nend_index = 1826\n\n# train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebefa9d915e0071a3a81a586156e7053a3430f4c"},"cell_type":"code","source":"holiday = pd.read_csv('../input/holiday/USholidays.csv',header=None, names = ['date', 'holiday'])\nholiday['date'] = pd.to_datetime(holiday['date'], yearfirst = True, format = '%y/%m/%d')\nholiday.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bb6e00ff4aac9f0a458c39f1b4419a31cd9e14f"},"cell_type":"code","source":"train_df = train_df.merge(holiday, how='left', on='date')\ntrain_df['holiday_bool'] = pd.notnull(train_df['holiday']).astype(int)\ntrain_df = pd.get_dummies(train_df, columns = ['month','holiday','weekday'] , prefix = ['month','holiday','weekday'])\n# train_df.head()\n# train_df.shape\n# train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fd8c8f69d71774306ea109fe8ab5d0608c0f9c4"},"cell_type":"code","source":"ext_var_list = ['date','year', 'day', 'holiday_bool',\n       'month_1', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12',\n       'holiday_Christmas Day', 'holiday_Columbus Day',\n       'holiday_Independence Day', 'holiday_Labor Day',\n       'holiday_Martin Luther King Jr. Day', 'holiday_Memorial Day',\n       'holiday_New Year Day', 'holiday_Presidents Day (Washingtons Birthday)',\n       'holiday_Thanksgiving Day', 'holiday_Veterans Day', 'weekday_0',\n       'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5',\n       'weekday_6']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f743c7794271b539dfc0d5cc2a3c77fb54f3023f"},"cell_type":"code","source":"exog_data = train_df[ext_var_list]\nexog_data = exog_data.set_index('date')\nexog_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"babfb4ac22cc03014095f14568a557b266c3a40a"},"cell_type":"code","source":"train_df = train_df.set_index('date')\n# train_df = train_df.reset_index()\ntrain_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f1be98a545131cb735e45e19fb7f48a3607f428"},"cell_type":"code","source":"start_index = '2017-10-01'\nend_index = '2017-12-31'\n# exog_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73d0cc2b1016a7ef2f80ad6426b168b197fd40cc"},"cell_type":"code","source":"%%time\nsarimax_mod6 = sm.tsa.statespace.SARIMAX(endog = train_df.sales[:start_index],\n                                        exog = exog_data[:start_index],  \n                                        trend='n', order=(6,1,0), seasonal_order=(0,1,1,7)).fit()\nprint(sarimax_mod6.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43afe8c5eeac3781b1f7e63f8beb8692adee184c"},"cell_type":"markdown","source":"These model coefficients are not very reliable as most of them are not significant. This would imply a high collinearity between the data.  "},{"metadata":{"trusted":true,"_uuid":"3d83f4612f1946f26efea8237884fae9389f64f5"},"cell_type":"code","source":"start_index = '2017-10-01'\nend_index = '2017-12-30'\nend_index1 = '2017-12-31'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6b55ca8c50d0f6fdd2f30776879e037065fe6c6e"},"cell_type":"code","source":"sarimax_mod6.forecast(steps = 121,exog = exog_data[start_index:end_index])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebe9de71af1fb60d524a93c2726ec7c25b0cd2dd"},"cell_type":"code","source":"train_df['forecast'] = sarimax_mod6.predict(start = pd.to_datetime(start_index), end= pd.to_datetime(end_index1),\n                                            exog = exog_data[start_index:end_index], \n                                            dynamic= True)  \n\ntrain_df[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df633ab7c0e51527d0b18b435d8ad3647a1340fb"},"cell_type":"code","source":"smape_kun(train_df[start_index:end_index]['sales'],train_df[start_index:end_index]['forecast'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91793f6a8a7b79cefdc76940de9645b367dabbc9"},"cell_type":"markdown","source":"### Some last words:\n\nARIMA makes much more sense to me now. ACF and PACF are useful to determine the p, d, q. And each test is indeed helping me to justify whether I'm getting a better model or worse one. \n\n\nPros:  \n* Intepretability: Each coefficient means a specific thing\n* ts key elements understanding: the concept of lags, and error lag terms are very unique, ARIMA gave a comprehensive cover on them. So even in the future I want to try some other regression model. I would add the lag terms and consider the error term. \n\nCons:  \n* Inefficiency: ARIMA needs to be run on each time series, since we have 500 store/item combinations, it needs to run 500 times. Every time we want to forecast the future, say on Jan 2, 2018, we want to forecast next 90 days. We need to re-run ARIMA. \n\n"},{"metadata":{"trusted":true,"_uuid":"e771c165aecb8039f2d864fbfa827b343ee5305e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}